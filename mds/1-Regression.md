# Regression

回归分析输出的是**标量(scalar)**，需要找到一个合适的函数使其能够预测出尽可能准确的标量值（函数值）。例如自动驾驶中各种传感器传入的数据就是input，输出的标量值则是方向盘旋转角度。



本节以`预测宝可梦进化后的CP值`(战斗能力)为例，宝可梦本身用$x$表示，其进化前的CP值用$x_{cp}$表示，种类用$x_s$表示，hp(血量)用$x_{hp}$表示，还有其他参数等；进化后的真实CP值用$\hat{y}$表示，预测CP值用$y$表示。而单个的宝可梦加上上标i即可。

## 选择模型

机器学习的`模型`是用参数表示的一组函数$f$，例如$y=b+wx_{cp}$，这个函数有两个参数$w(weight),b(bias)$，自变量仅是$x_{cp}$，那么这个函数可以表示$w,b$取任意值的一组函数，它就构成一个模型。机器的目的就是在这些函数中找到最好的一个来输出预测。

## 判断函数好坏

机器判断函数好坏是通过训练+调整的方式。当确定好了模型用哪些参数和自变量表示之后：

1. 选一定量的真实数据作为**训练集**(宝可梦$x^1,x^2,\dots,x^n$和其真实的的进化后CP值$\hat{y}^1,\hat{y}^2,\dots,\hat{y}^n$)
2. 对模型中的每一个函数$f_i$，将训练集中的自变量($x_{cp}$等)代入，计算出该函数得到的所有预测CP值$y_1,y_2,\dots,y^n$。
3. 定义一个`损失函数(Loss Fuction)`，用来评判函数$f_i$的预测结果好坏。例如我们可以用方差值来评判，即$L(f) = \sum^{10}_{n=1}(\hat{y}^n-f(x_{cp}^n))^2$（训练集有10个宝可梦）。(线代可以直接解?)
4. 计算出每个函数$f_i$在训练集上的损失函数值，那么损失函数值最小的函数就是我们要找的最好的函数。即$f^*=arg\ min_f L(f)$或者$w^*,b^*=arg\ min_{w,b}L(w,b)$

## 找最好的函数

将损失函数作图，也许你一眼就能看出最低点，或者利用一些数学解法，也能很快找到，但是有什么通用的方法让机器能够找到最小的那个呢？我们可以采用`梯度下降(Gradient Descent)`的方法：(配合图示)

1. 随机找一个$w$值：$w^0$，计算损失函数在该处的切线斜率: $\frac{dL}{dw}|_{w=w^0}$
2. 当斜率为负，往右走才能找到最低点，因此新的$w^1 = w^0 - \eta \frac{dL}{dw}|_{w=w^0}$，其中$\eta$称之为`学习率(Learning Rate)`；斜率为正亦然。
3. 往复进行下去，直到斜率为0，则已经找到了该方法认为的最小值点。但这个值有可能仅是局部最小(Local Minimum)而非全局最小(Global Minimum)，当Loss函数是凸函数的时候，这个值就是全局最小。

![](..\imgs\2.png)

上面是仅有一个参数$w$的方法，那么有多个参数呢？——在随机点上，让每个参数对$L$求偏导，再根据学习率更新即可。以2个参数为例，即：
$$
w^1 = w^0 - \eta\frac{\partial L}{\partial w}|_{w=w^0,b=b^0}\\
b^1 = b^0 - \eta\frac{\partial L}{\partial b}|_{w=w^0,b=b^0}
$$
对应的等高线如图所示，每次更新，就是在椭圆上做该点的法线。

![](..\imgs\3.png)

## 结果

### 单自变量

当我们的模型是$y=b+wx_{cp}$，找到的最好的函数是$y=-188.4+2.7x_{cp}$. 此时**训练集**的预测值和真实值之间差的平均值(`训练误差`)是31.9，而**测试集**(`测试误差`)是35.0，如图所示：

![](..\imgs\4.png)

这显然不够好，那当我们的模型是$y=b+w_1x_{cp}+w_2(x_{cp})^2$呢？此时最好的函数是$y=-10.3+x_{cp}+2.7\times10^{-3}(x_{cp})^2$，那么这时候的**训练误差**是15.4，**测试误差**是18.4.

同理，当我们增加三次项的分析，得到的**训练误差**是15.3，**测试误差**是18.1. 因为我们每增加一次，后面的函数组范围总是包含前面的函数组，那么就能找到在前面的基础上更好的函数。



可是当我们增加到四次项、五次项的时候，**训练误差**在逐渐减小，**测试误差**却逐渐增大并且十分夸张：

![](..\imgs\5.png)



尽管我们一直在找训练集上更好的函数，但是这个函数在测试集上的表现却并不一定更好，这种现象叫做`过拟合(Overfitting)`。因此我们不一定要追求复杂的模型，而要找到最合适的模型。

### 多自变量

很显然，宝可梦的进化后CP值还会和宝可梦的种类有关，不同种类的曲线可能不同；当是Pidgey时，曲线是$y=b1+w_1x_{cp}$,...,以此类推。得到如下的公式，此时的**训练误差**是3.8，**测试误差**是14.3.

<img src="..\imgs\6.png" style="zoom:150%;" />

同样，我们也可以在此基础上引入$x_{cp}^2$，再来查看误差结果。



我们还有什么办法能够减少误差吗？

可以在原来的Loss定义上，加上一个`正则项(Regularization term)`：函数是$y=b+\sum w_ix_i$，那么损失函数是$L=\sum_n(\hat{y}^n-(b+\sum w_ix_i))^2+\lambda \sum(w_i)^2$，当整个Loss Function越小，即方差和正则项越小，所选择的函数越好。正则项的作用是，保证参数$w$足够小，这样函数对输入的变化更不敏感，输出的变化就不会太多，整个函数越`平滑`。

其中$\lambda$是我们可以手动调的参数，如图，当$\lambda$逐渐增加，也就意味着正则项对loss的影响力越大，选择的最优函数会考虑$w$小的，函数会越平滑。此时**训练误差**会逐渐上升，因为当正则项影响越大，对$w$的选择就越偏离没有正则项时的$w$了；而**测试误差**却不一定会上升。图中可以看到测试误差降到一个点后增加，由此可见，我们也不希望出现太平滑的曲线(一条直线).

![](..\imgs\7.png)















