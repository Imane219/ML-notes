# Gradient Descent

`梯度下降法`用来解决如下优化问题，求$\theta^*$
$$
\theta^* = \arg\min\limits_\theta L(\theta)
$$
$L$: Loss function  $\theta$:parameters



具体过程如下：

![](..\imgs\11.png)

## 学习率$\eta$

学习率决定了下降的步长，如果步长过大，有可能会跨过最低点，越优化损失函数越大；步长过小则下降速率很慢。因此一个简单的做法是**让$\eta$随着走的步数(时间)变小**：$\eta^t = \frac{\eta}{\sqrt{t+1}}$. 一开始距离最低点较远，可以步长大一些；当离最低点越来越近则需要更加谨慎。

### Adagrad

对每个参数采用不同的学习率，对于参数$w$而言，其值的更新如下：
$$
w^{t+1} = w^t - \frac{\eta^t}{\sigma^t}g^t\\
\eta^t = \frac{\eta}{\sqrt{t+1}}, g^t=\frac{\partial L(\theta^t)}{\partial w}, \sigma^t=\sqrt{\frac{1}{t+1}\sum\limits^t_{i=0}(g^i)^2}
$$
其中，$\eta^t$是随时间更新的学习率，$g^t$是第$t$次更新的参数$w$微分值，$\sigma^t$是前$t$次的微分值的**root mean square**.化简后结果如下：
$$
w^{t+1} = w^t - \frac{\eta}{\sqrt{\sum^t_{i=0}(g^i)^2}}g^t
$$
**理解**：以二次式为例，一个最好的步长是一步直接跨到最低点，此时步长值为：**一次微分/二次微分**。但是二次微分计算量比较大，采用一次微分平方再开根的方式来模拟二次微分值。

![](..\imgs\12.png)

![](..\imgs\13.png)

## 随机梯度下降(Stochastic Gradient Descent)

**梯度下降法**在计算损失函数的时候，先计算完训练集中全部样本的损失之和，再求模型的参数梯度进行更新，公式如下所示：
$$
L=\sum_n(\hat{y}^n-(b+\sum w_ix_i^n))^2,\ \theta^i=\theta^{i-1}-\eta \nabla L(\theta^{i-1})
$$
**随机梯度下降法**则是每计算一个样本的损失，就进行一次参数更新，公式如下：
$$
L^n=(\hat{y}^n-(b+\sum w_ix_i^n))^2,\ \theta^i=\theta^{i-1}-\eta \nabla L^n(\theta^{i-1})
$$
随机梯度下降法对于梯度更新来说会更快。

## 特征缩放(Feature Scaling)

对于每个输入的特征（图片的3个维度可以是3个特征），他们的值往往有差异，这种差异造成的loss在不同方向上的梯度变化不同，进而影响梯度下降法的实施。如下图蓝色的loss面，在梯度下降时需要不断调整步幅。而对特征进行归一化后，则会让学习变得更容易（如下图绿色loss面）

![](..\imgs\14.png)

**标准化/z值归一化**——对每张图片都进行减去均值除以标准差的操作，这样每个特征值都是均值为0标准差为1的正态分布。
$$
x'=\frac{x-\bar{x}}{\sigma}
$$
其中x可以有多个维度（特征），表示对每个维度（特征）都进行归一化。均值和标准差表示数据集所有图片该个特征值的平均值和标准差。

## 梯度下降的限制

1. **local minimum:**局部最小值的偏微分是0，会导致步长为0，梯度下降法不再继续往前走。
2. **seddle point:**鞍点的偏微分也是0，同上。

